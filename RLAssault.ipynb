{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RLAssault.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naC8AcNOgvw3"
      },
      "source": [
        "# Assault (Atari 2600): which is the best agent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ9dQTILgvw4"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qza5w1gFo4u2"
      },
      "source": [
        "!pip install imageio\n",
        "!pip install gym\n",
        "!pip install gym[all]\n",
        "!pip install gym[atari]\n",
        "!pip install imageio-ffmpeg\n",
        "!pip3 install matplotlib\n",
        "!pip install tensorflow_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKgvxlPen7Lm"
      },
      "source": [
        "! wget http://www.atarimania.com/roms/Roms.rar && unrar x Roms.rar && unzip Roms/ROMS.zip\n",
        "! python3 -m atari_py.import_roms ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZFq8D3zgvw6"
      },
      "source": [
        "### Import "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSqbHTsTo4u5"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import imageio\n",
        "import IPython\n",
        "import base64\n",
        "import gym\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import clear_output\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "if tf.__version__ > \"2.4.0\":\n",
        "    import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjgIRq3Tgvw7"
      },
      "source": [
        "### Load Game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYBX5-S3o4u6",
        "outputId": "9bbebc1c-3b51-4c8d-b22b-a406e288c9bc"
      },
      "source": [
        "environment = gym.make(\"Assault-v4\")\n",
        "\n",
        "print('Number of states: {}'.format(environment.observation_space))\n",
        "print('Number of actions: {}'.format(environment.action_space))\n",
        "print(environment.unwrapped.get_action_meanings())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of states: Box(0, 255, (210, 160, 3), uint8)\n",
            "Number of actions: Discrete(7)\n",
            "['NOOP', 'FIRE', 'UP', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFZjC3p4gvw8"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEvtqd45o4u7"
      },
      "source": [
        "def image_preprocess_observations(frame, shape=(200, 160)):\n",
        "    frame = frame.astype(np.uint8)  # cv2 requires np.uint8, other dtypes will not work\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    frame = frame[10:, :160]  # crop image\n",
        "    #frame = cv2.resize(frame, shape, interpolation=cv2.INTER_NEAREST)\n",
        "    frame = frame.reshape((*shape))\n",
        "\n",
        "    return frame\n",
        "\n",
        "\n",
        "def create_video(env, model, video_filename = 'imageio'):\n",
        "  def embed_mp4(filename):\n",
        "    \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "      <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "\n",
        "  num_episodes = 2\n",
        "  video_filename = video_filename + \".mp4\"\n",
        "  with imageio.get_writer(video_filename, fps=60) as video:\n",
        "    for _ in range(num_episodes):\n",
        "        observation = environment.reset()\n",
        "\n",
        "        state = image_preprocess_observations(observation)\n",
        "        #take the firsts 4 step as init\n",
        "        states = [state, state, state, state]\n",
        "            \n",
        "        terminated = False\n",
        "        video.append_data(observation)\n",
        "        while not terminated:\n",
        "            agent_states = np.expand_dims(np.array(tf.cast(states, tf.float32)).reshape(model.state_size), axis=0)\n",
        "            action = model.act(agent_states)\n",
        "            #print(agent_states.shape)\n",
        "            env_action = action #agent.atari_action_from_output(action)  # UP=2 DOWN=3\n",
        "\n",
        "            # Take action    \n",
        "            observation, _, terminated, _ = environment.step(env_action) \n",
        "            next_state = image_preprocess_observations(observation)\n",
        "\n",
        "            next_states = states[1:]\n",
        "            next_states.append(next_state)\n",
        "            states = next_states\n",
        "            #agent.store(agent_states, action, reward, np.array(next_states).reshape((80,80,4)), terminated)\n",
        "            video.append_data(observation)\n",
        "\n",
        "  embed_mp4(video_filename)\n",
        "\n",
        "\n",
        "def play_games(environment, agent):\n",
        "    print(agent)\n",
        "    total_reward = 0\n",
        "    for i in range(4):\n",
        "        observation = environment.reset()\n",
        "        observation, _, _, _ = environment.step(1)\n",
        "\n",
        "        state = image_preprocess_observations(observation)\n",
        "        #take the firsts 4 step as init\n",
        "        states = [state, state, state, state]\n",
        "            \n",
        "        terminated = False\n",
        "        print(\"Start game {}\".format(i))\n",
        "        while not terminated:\n",
        "            agent_states = np.expand_dims(np.array(tf.cast(states, tf.float32)).reshape(agent.state_size), axis=0)\n",
        "            env_action = agent.act(agent_states)\n",
        "\n",
        "            # Take action  \n",
        "            #print(env_action)\n",
        "            observation, reward, terminated, _ = environment.step(env_action) \n",
        "            next_state = image_preprocess_observations(observation)\n",
        "\n",
        "            next_states = states[1:]\n",
        "            next_states.append(next_state)\n",
        "            states = next_states\n",
        "\n",
        "            total_reward += reward\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def self_play(best_agent_reward, current_agent, environment):\n",
        "    #best_agent_reward = play_games(environment, best_agent)\n",
        "    current_agent_reward = play_games(environment, current_agent)\n",
        "    print(\"Best reward {}\".format(best_agent_reward))\n",
        "    if current_agent_reward > best_agent_reward:\n",
        "        print(\"Better model found! Saving best_model\")\n",
        "        current_agent.save_model(0, \"Break_best_model\")\n",
        "        return current_agent_reward\n",
        "    return current_agent_reward\n",
        "\n",
        "\n",
        "def plot_bar(data, n, filename=\"Bar\"):\n",
        "    plt.figure()\n",
        "    plt.bar(np.arange(n), data, align='center', alpha=0.5)\n",
        "    plt.xlabel('Actions')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.xticks(np.arange(7), ['NOOP', 'FIRE', \"UP\", 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'])\n",
        "    plt.title('Histogram of Actions choosen')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "def plot_graph(data, filename=\"Graph\"):\n",
        "    plt.figure()\n",
        "    plt.plot(data)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.xticks(np.arange(len(data)))\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "def plot_mult_bar(data1, data2, n, filename=\"MultiBar\"):\n",
        "    plt.figure()\n",
        "    w = 0.3\n",
        "    plt.bar(np.arange(n), data1, width=w, align='center', alpha=0.5)\n",
        "    plt.bar(np.arange(n), data2, width=w, align='center', alpha=0.5)\n",
        "    plt.xlabel('Actions')\n",
        "    plt.ylabel('Counts')\n",
        "    plt.xticks(np.arange(7), ['NOOP', 'FIRE', \"UP\", 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE'])\n",
        "    plt.title('Histogram of Actions choosen')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "    plt.close()\n",
        "    \n",
        "    \n",
        "def plot_multi_graph(data1, data2, filename=\"MultiGraph\"):\n",
        "    plt.figure()\n",
        "    plt.plot(data1)\n",
        "    plt.plot(data2)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Rewards')\n",
        "    plt.xticks(np.arange(len(data1)))\n",
        "    plt.title('Episode Rewards')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(filename + \".png\")\n",
        "    plt.close()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnwQ28fIgvw9"
      },
      "source": [
        "### DQN Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4lEfkbQOxhm"
      },
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, n_episode, greedy=False):\n",
        "        # Initialize atributes\n",
        "        # Stack four images preprecessed \n",
        "        self.state_size = (200, 160, 4)  # environment.observation_space.shape*4\n",
        "        self.action_size = 7  # environment.action_space.n\n",
        "        self.greedy = greedy\n",
        "\n",
        "        #self.optimizer = Adam(learning_rate=0.000001)\n",
        "        self.optimizer = RMSprop(learning_rate=0.00025,\n",
        "                                       decay=0.95,\n",
        "                                       momentum=0.0,\n",
        "                                       epsilon=0.00001,\n",
        "                                       centered=True)\n",
        "        self.n_episode = n_episode\n",
        "        self.batch_size = 32\n",
        "        \n",
        "        self.expirience_replay = deque(maxlen=5000)\n",
        "        \n",
        "        # Initialize discount and exploration rate\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon_init = 0.1 if self.greedy else 0.1\n",
        "        self.epsilon_final = 0.01\n",
        "        self.epsilon = np.logspace(self.epsilon_init, self.epsilon_final, self.n_episode, endpoint=True)\n",
        "        # esploriation term for UCB and counter for the actions\n",
        "        # self.ucb_c = 2 \n",
        "        # self.counter_actions = np.ones(self.action_size)  # use ones and not zeros because the fraction in UCB\n",
        "        \n",
        "        # Build networks\n",
        "        self.q_network = self._build_compile_model()\n",
        "        self.target_network = self._build_compile_model()\n",
        "        self.align_target_model()\n",
        "\n",
        "        self.best_agent_reward = 0\n",
        "\n",
        "    def store(self, state, action, reward, next_state, terminated):\n",
        "        self.expirience_replay.append((state, action, reward, next_state, terminated))\n",
        "    \n",
        "    def _build_compile_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(16, (8, 8), strides=(4, 4), padding='same', activation='relu', input_shape=self.state_size))\n",
        "        model.add(Conv2D(32, (4, 4), strides=(2, 2), activation='relu'))\n",
        "        # model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        # initializer = tf.keras.initializers.Constant(10.)\n",
        "        # model.add(Dense(self.action_size, activation='linear', kernel_initializer=initializer))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "\n",
        "        # compile the model using traditional Machine Learning losses and optimizers\n",
        "        model.compile(loss=tf.keras.losses.Huber(), optimizer=self.optimizer, metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def align_target_model(self):\n",
        "        print(\"Aligning models..\")\n",
        "        self.target_network.set_weights(self.q_network.get_weights())\n",
        "        \n",
        "    def save_model(self, episode, file_name=None):\n",
        "        print(\"Saving weights...\")\n",
        "        if file_name is None:\n",
        "            file_name = 'DQN_net_weights-' + str(episode) + '-' + str(self.best_agent_reward) + '-'+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.q_network.save_weights(file_name + '.h5')\n",
        "        return file_name\n",
        "    \n",
        "    def load_model(self, filename):\n",
        "        # load pre-trained model if exist\n",
        "        if (os.path.isfile(filename + '.h5')):\n",
        "            print(\"Loading previous weights: {}\".format(filename))\n",
        "            self.q_network.load_weights(filename + '.h5')\n",
        "            self.align_target_model()\n",
        "    \n",
        "    def act(self, state, episode=None):\n",
        "        if episode:\n",
        "            # training mode with UCB or epsilon greedy decaying\n",
        "            # Upper-Confidence-Bound policy\n",
        "            #ucb_weights = self.ucb_c*np.sqrt(np.log(episode)/self.counter_actions)\n",
        "            #return np.argmax(self.q_network.predict(state)[0] + ucb_weights)\n",
        "            \n",
        "            # epsilon greedy with epsilon decaying\n",
        "            if np.random.rand() <= self.epsilon[episode]:\n",
        "                # print(\"Epsilon\")\n",
        "                return random.randint(0, self.action_size-1)\n",
        "\n",
        "        else:\n",
        "            # if epsilon is None we are in play not training, so fixed epsilon-greedy\n",
        "            if np.random.rand() <= self.epsilon_final:\n",
        "                # print(\"Epsilon\")\n",
        "                return random.randint(0, self.action_size-1)\n",
        "        # print(\"POLICY\")\n",
        "        return np.argmax(self.q_network.predict(state)[0])\n",
        "\n",
        "    def train_on_single(self):\n",
        "        if len(self.expirience_replay) < self.batch_size:\n",
        "            return None\n",
        "        \n",
        "        print(\"Train on single..\")\n",
        "        minibatch = random.sample(self.expirience_replay, self.batch_size)\n",
        "        \n",
        "        # extract SARS tuples\n",
        "        for state, action, reward, next_state, terminated in minibatch:\n",
        "            # predict values\n",
        "            target = self.q_network.predict(np.expand_dims(state, axis=0))\n",
        "\n",
        "            if terminated:\n",
        "                # if last state there's no future actions\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                # take value for next state \n",
        "                t = self.target_network.predict(np.expand_dims(next_state, axis=0))\n",
        "                # update the taget with the max q-value of next state\n",
        "                # using a greedy policy\n",
        "                target[0][action] = reward + self.gamma * np.max(t, axis=1)\n",
        "\n",
        "            self.q_network.fit(np.expand_dims(state, axis=0), target, epochs=1, verbose=0)\n",
        "    \n",
        "    def get_arrays_from_batch(self, batch):\n",
        "        try:\n",
        "            states = np.array([x[0] for x in batch])\n",
        "            actions = np.array([x[1] for x in batch])\n",
        "            rewards = np.array([x[2] for x in batch])\n",
        "            next_states = np.array([x[3] for x in batch])\n",
        "            terminateds = np.array([x[4] for x in batch])\n",
        "        except:\n",
        "            states = x[0]\n",
        "            actions = x[1]\n",
        "            rewards = x[2]\n",
        "            next_states = x[3]\n",
        "            terminateds = x[4]\n",
        "        \n",
        "        return states, actions, rewards, next_states, terminateds\n",
        "    \n",
        "    def train_on_batch(self):\n",
        "        if len(self.expirience_replay) < self.batch_size:\n",
        "            return None\n",
        "            \n",
        "        #print(\"Train on batch..\")\n",
        "        minibatch = random.sample(self.expirience_replay, self.batch_size)\n",
        "        # get the SARS tuple\n",
        "        states, actions, rewards, next_states, terminateds = self.get_arrays_from_batch(minibatch)\n",
        "       \n",
        "        # predict values for next states using target net\n",
        "        next_Q_values = self.target_network.predict(next_states)\n",
        "        # takes the greedy actions\n",
        "        max_next_Q_values = np.max(next_Q_values, axis=1)\n",
        "\n",
        "        # calculate target values for training, not for the last state\n",
        "        target_Q_values = (rewards + (1 - terminateds) * self.gamma * max_next_Q_values)\n",
        "        target_Q_values = target_Q_values.reshape(-1, 1)\n",
        "        \n",
        "        mask = tf.one_hot(actions, self.action_size)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            # takes values predicted for current states\n",
        "            all_Q_values = self.q_network(states)\n",
        "            # takes values only for given actions\n",
        "            Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
        "            # loss = tf.reduce_mean(tf.keras.losses.Huber()(target_Q_values, Q_values))\n",
        "            loss = tf.reduce_mean(tf.keras.losses.MeanSquaredError()(target_Q_values, Q_values))\n",
        "\n",
        "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def play(self, environment, num_of_episodes, timesteps_per_episode, train=True):\n",
        "        cont_e = 0\n",
        "        total_reward_list = []\n",
        "        total_actions_list = []\n",
        "        for e in range(0, num_of_episodes):\n",
        "            print(\"Episode {} start...\".format(e))\n",
        "            reward_list = []\n",
        "            actions_list = []\n",
        "            # Reset the environment\n",
        "            state = environment.reset()\n",
        "\n",
        "            state = image_preprocess_observations(state)\n",
        "            # take the firsts 4 step as init\n",
        "            states = np.stack([state] * 4, axis = 2)\n",
        "\n",
        "            # Initialize variables\n",
        "            episode_reward = 0\n",
        "            terminated = False\n",
        "            experience_replay_temp = []\n",
        "\n",
        "            for timestep in range(timesteps_per_episode):\n",
        "                # Run Action\n",
        "                if self.greedy:\n",
        "                  env_action = self.act(np.expand_dims(states, axis=0))\n",
        "                else:\n",
        "                  env_action = self.act(np.expand_dims(states, axis=0), e)\n",
        "                # +1 on action counter for UCB\n",
        "                # self.counter_actions[env_action] += 1\n",
        "                \n",
        "                # Take action    \n",
        "                next_state, reward, terminated, info = environment.step(env_action) \n",
        "                # print(\"Reward obtained by q_net \" + str(reward))\n",
        "                next_state = image_preprocess_observations(next_state)\n",
        "                next_states = np.append(states[:, :, 1: ], np.expand_dims(next_state, 2), axis = 2)\n",
        "                \n",
        "                self.store(states, env_action, reward, next_states, terminated)\n",
        "\n",
        "                # train after some timestep, less become too much computational effort\n",
        "                if cont_e > 100 and train:\n",
        "                    # print(\"start timestamp for training: {}\".format(str(timestep)))\n",
        "                    # loss = self.train_on_single()\n",
        "                    loss = self.train_on_batch()\n",
        "                    # print(\"loss: \" + str(loss))\n",
        "                    cont_e = 0\n",
        "\n",
        "                cont_e += 1\n",
        "                states = next_states\n",
        "\n",
        "                # save stats\n",
        "                reward_list.append(reward)\n",
        "                actions_list.append(env_action)\n",
        "                episode_reward += reward\n",
        "                total_actions_list.append(env_action)\n",
        "\n",
        "                if terminated:\n",
        "                    print(\"The rewards for the episode {} after {} timestep is {}\".format(e, timestep, episode_reward))\n",
        "                    total_reward_list.append(episode_reward)\n",
        "\n",
        "                    # DQN training at the end of the episode\n",
        "                    if train:\n",
        "                        loss = self.train_on_batch()\n",
        "                        print(\"Terminated loss: \" + str(loss))\n",
        "                        cont_e = 0\n",
        "\n",
        "                    print(\"____________END__________________\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            if (e + 1) % 20 == 0:\n",
        "                print(\"***************ALIGN-MODEL*******************\")\n",
        "                if train:\n",
        "                    self.align_target_model()\n",
        "                print(\"**********************************\")\n",
        "            if (e + 1) % 100 == 0:\n",
        "                print(\"***************SAVE-WEIGHTS*******************\")\n",
        "                new_best_agent_reward = self_play(self.best_agent_reward, self, environment)\n",
        "                if new_best_agent_reward > self.best_agent_reward:\n",
        "                    self.best_agent_reward = new_best_agent_reward\n",
        "                    self.save_model(e)\n",
        "                print(\"the score is \" + str(new_best_agent_reward))\n",
        "                print(\"**********************************\")\n",
        "\n",
        "        return total_reward_list, total_actions_list\n",
        "      "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48mCZ8Mpgvw-"
      },
      "source": [
        "### Actor-Critic Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khy1xHapgvw_"
      },
      "source": [
        "class Model(tf.keras.Model):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "    \n",
        "        # model blocks\n",
        "        self.conv_1 = Conv2D(16, (8, 8), strides=(4,4), padding='same', activation='relu', input_shape=state_size)\n",
        "        self.conv_2 = Conv2D(32, (4, 4), strides=(2,2), activation='relu')\n",
        "        self.dense = Dense(256, activation='relu')\n",
        "        self.dense_v = Dense(128, activation='relu')\n",
        "        self.dense_a = Dense(128, activation='relu')\n",
        "        self.out_v = Dense(1, activation=None)\n",
        "        self.out_a = Dense(self.action_size, activation='softmax')\n",
        "\n",
        "    def call(self, input_data):\n",
        "        x = self.conv_1(input_data)\n",
        "        x = self.conv_2(x)\n",
        "        x = Flatten()(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "        x_a = self.dense_a(x)\n",
        "        a = self.out_a(x_a)\n",
        "\n",
        "        x_v = self.dense_v(x)\n",
        "        v = self.out_v(x_v)\n",
        "        return v, a\n",
        "    \n",
        "\n",
        "class ACAgent():\n",
        "    def __init__(self, gamma = 0.99):\n",
        "        self.state_size = (200, 160, 4)\n",
        "        self.action_size = 7\n",
        "        self.gamma = gamma\n",
        "        self.optimizer = Adam(learning_rate=1e-6)\n",
        "        m = Model(self.state_size, self.action_size)\n",
        "        m.compile(loss=tf.keras.losses.Huber(), optimizer=self.optimizer, metrics=['accuracy'])\n",
        "        m.build((None, 200, 160, 4))\n",
        "        self.model = m\n",
        "      \n",
        "    def save_model(self, episode, file_name=None):\n",
        "        print(\"Saving weights...\")\n",
        "        if file_name is None:\n",
        "            file_name = 'AC_net_weights-' + str(episode) + '-'+ str(self.best_agent_reward) + '-'+ datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        self.model.save_weights(file_name + '.h5')\n",
        "        return file_name\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        # load model if exist\n",
        "        if (os.path.isfile(filename + '.h5')):\n",
        "            print(\"Loading previous weights: {}\".format(filename))\n",
        "            self.model.load_weights(filename + '.h5')\n",
        "    \n",
        "    def act(self, state, e=None):\n",
        "        _, prob = self.model(state)\n",
        "        prob = prob.numpy()\n",
        "        # create a distribution to sample\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        action = dist.sample()\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "    def actor_loss_single(self, prob, action, td):\n",
        "        # create a distribution\n",
        "        dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "        log_prob = dist.log_prob(action)\n",
        "        # error is td * ln_prob\n",
        "        loss = -log_prob*td\n",
        "        return loss    \n",
        "         \n",
        "    def train_on_single(self, state, action, reward, next_state, done):\n",
        "        # create batches of 1\n",
        "        state = np.array([state])\n",
        "        next_state = np.array([next_state])\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # value and action for the state St\n",
        "            v, p =  self.model(state, training=True)\n",
        "            # value and action for the state St+1\n",
        "            vn, _ = self.model(next_state, training=True)\n",
        "            # td error\n",
        "            td = reward + self.gamma * vn * (1 - int(done)) - v\n",
        "\n",
        "            a_loss = self.actor_loss_single(p, action, td)\n",
        "            c_loss = td**2  # to reproduce a MSE with only one data\n",
        "            total_loss = a_loss + 0.4 * c_loss\n",
        "          \n",
        "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return total_loss\n",
        "\n",
        "    def get_discounted_rewards(self, rewards):\n",
        "      discounted_rewards = []\n",
        "      sum_reward = 0\n",
        "      rewards.reverse()\n",
        "      for r in rewards:\n",
        "        sum_reward = r + self.gamma * sum_reward\n",
        "        discounted_rewards.append(sum_reward)\n",
        "      discounted_rewards.reverse()\n",
        "\n",
        "      return discounted_rewards\n",
        "\n",
        "    def actor_loss_on_batch(self, probs, actions, td):\n",
        "        \n",
        "        probability = []\n",
        "        log_probability= []\n",
        "        for pb,a in zip(probs,actions):\n",
        "          dist = tfp.distributions.Categorical(probs=pb, dtype=tf.float32)\n",
        "          log_prob = dist.log_prob(a)\n",
        "          prob = dist.prob(a)\n",
        "          probability.append(prob)\n",
        "          log_probability.append(log_prob)\n",
        "\n",
        "        # print(probability)\n",
        "        # print(log_probability)\n",
        "\n",
        "        p_loss= []\n",
        "        e_loss = []\n",
        "        td = td.numpy()\n",
        "\n",
        "        for pb, t, lpb in zip(probability, td, log_probability):\n",
        "          t =  tf.constant(t)\n",
        "          policy_loss = tf.math.multiply(lpb,t)\n",
        "          entropy_loss = tf.math.negative(tf.math.multiply(pb,lpb))\n",
        "          p_loss.append(policy_loss)\n",
        "          e_loss.append(entropy_loss)\n",
        "        p_loss = tf.stack(p_loss)\n",
        "        e_loss = tf.stack(e_loss)\n",
        "        p_loss = tf.reduce_mean(p_loss)\n",
        "        e_loss = tf.reduce_mean(e_loss)\n",
        "\n",
        "        loss = -p_loss - 0.0001 * e_loss\n",
        "        return loss\n",
        "\n",
        "    def train_on_batch(self, states, actions, discounted_rewards):\n",
        "        states = np.array(states, dtype=np.float32)\n",
        "        actions = np.array(actions, dtype=np.int32)\n",
        "        discounted_rewards = np.array(discounted_rewards, dtype=np.float32)\n",
        "        discounted_rewards = tf.reshape(discounted_rewards, (len(discounted_rewards),))\n",
        "        print(states.shape)\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            v, p = self.model(states, training=True)\n",
        "            v = tf.reshape(v, (len(v),))\n",
        "            td = tf.math.subtract(discounted_rewards, v)\n",
        "            a_loss = self.actor_loss_on_batch(p, actions, td)\n",
        "            c_loss = 0.5*tf.keras.losses.mean_squared_error(discounted_rewards, v)\n",
        "            total_loss = a_loss + 0.4 * c_loss\n",
        "          \n",
        "        grads = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "        return a_loss, c_loss\n",
        "\n",
        "    def play(self, environment, num_of_episodes, timesteps_per_episode, train=True):\n",
        "      cont_e = 0\n",
        "      total_reward_list = []\n",
        "      total_actions_list = []\n",
        "      for e in range(0, num_of_episodes):\n",
        "          print(\"Episode {} start...\".format(e))\n",
        "          state_list = []\n",
        "          reward_list = []\n",
        "          actions_list = []\n",
        "          # Reset the environment\n",
        "          state = environment.reset()\n",
        "\n",
        "          state = image_preprocess_observations(state)\n",
        "          #take the firsts 4 step as init\n",
        "          states = tf.cast(np.stack([state] * 4, axis = 2), tf.float32)\n",
        "\n",
        "          # Initialize variables\n",
        "          episode_reward = 0\n",
        "          terminated = False\n",
        "\n",
        "          experience_replay_temp = []\n",
        "          timestep = -1\n",
        "          #for timestep in range(timesteps_per_episode):\n",
        "          while True:\n",
        "              timestep += 1\n",
        "              # Run Action\n",
        "              env_action = self.act(np.expand_dims(states, axis=0))\n",
        "\n",
        "              # Take action    \n",
        "              next_state, reward, terminated, info = environment.step(env_action) \n",
        "              #print(\"Reward obtained by q_net \" + str(reward))\n",
        "              next_state = image_preprocess_observations(next_state)\n",
        "              next_states = tf.cast(np.append(states[:, :, 1: ], np.expand_dims(next_state, 2), axis = 2), tf.float32)\n",
        "\n",
        "              if train:\n",
        "                  loss = self.train_on_single(states, env_action, reward, next_states, terminated)\n",
        "                  # print(\"loss: \" + str(loss))\n",
        "            \n",
        "              \n",
        "              state_list.append(states)\n",
        "              states = next_states\n",
        "              reward_list.append(reward)\n",
        "              actions_list.append(env_action)\n",
        "              episode_reward += reward\n",
        "              \n",
        "              total_actions_list.append(env_action)\n",
        "\n",
        "              if terminated:\n",
        "                  print(\"The rewards for the episode {} after {} timestep is {}\".format(e, timestep, episode_reward))\n",
        "                  total_reward_list.append(episode_reward)\n",
        "\n",
        "                  #if train:\n",
        "                  #  discounted_reward_list = self.get_discounted_rewards(reward_list)\n",
        "                  #  loss = self.train_on_batch(state_list, actions_list, discounted_reward_list)\n",
        "                  print(\"____________END__________________\")\n",
        "                  break\n",
        "\n",
        "          if (e + 1) % 50 == 0:\n",
        "              print(\"***************SAVE-WEIGHTS*******************\")\n",
        "              self.save_model(e)\n",
        "              print(\"**********************************\")\n",
        "\n",
        "      return total_reward_list, total_actions_list\n",
        "    "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nWey92dnRgL"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFyrNdRFo4u9"
      },
      "source": [
        "# play parameters\n",
        "num_of_episodes = 50000\n",
        "timesteps_per_episode = 5500\n",
        "cont_e = 0\n",
        "\n",
        "dqn_model_file = \"\"\n",
        "dqn_agent = DQNAgent(num_of_episodes, greedy=False)\n",
        "dqn_agent.load_model(dqn_model_file)\n",
        "\n",
        "ac_agent = ACAgent()\n",
        "ac_model_file = \"\"\n",
        "ac_agent.load_model(ac_model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XW9MeA59gvxB"
      },
      "source": [
        "total_reward_dqn, total_action_dqn = dqn_agent.play(environment, num_of_episodes, timesteps_per_episode, train=True)\n",
        "\n",
        "total_reward_a2c, total_action_a2c = ac_agent.play(environment, num_of_episodes, timesteps_per_episode, train=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzkWJOgnRgN"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp_wLkorLFuA"
      },
      "source": [
        "environment.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJxMO8z76XS"
      },
      "source": [
        "num_of_evaluation_step = 10\n",
        "timesteps_per_episode = 1500\n",
        "ac_loaded_model = ACAgent()\n",
        "ac_filename = \"7\"\n",
        "ac_loaded_model.load_model(ac_filename)\n",
        "\n",
        "total_reward_a2c, total_action_a2c = ac_loaded_model.play(environment, num_of_evaluation_step, timesteps_per_episode, train=False)\n",
        "actions_count_a2c = [total_action_a2c.count(0), total_action_a2c.count(1), total_action_a2c.count(2), total_action_a2c.count(3), total_action_a2c.count(4), total_action_a2c.count(5), total_action_a2c.count(6)]\n",
        "plot_bar(actions_count_a2c, len(actions_count_a2c), \"ActionsBarA2C\"+str(num_of_evaluation_step))\n",
        "plot_graph(total_reward_a2c, filename=\"RewardGraphA2C\"+str(num_of_evaluation_step))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZwxRvaVnRgO"
      },
      "source": [
        "num_of_evaluation_step = 10\n",
        "timesteps_per_episode = 5500\n",
        "\n",
        "dqn_agent = DQNAgent(num_of_evaluation_step, greedy=True)\n",
        "dqn_model_file = \"\"\n",
        "dqn_agent.load_model(dqn_model_file)\n",
        "\n",
        "total_reward_dqn, total_action_dqn = dqn_agent.play(environment, num_of_evaluation_step, timesteps_per_episode, train=False)\n",
        "actions_count_dqn = [total_action_dqn.count(0), total_action_dqn.count(1), total_action_dqn.count(2), total_action_dqn.count(3), total_action_dqn.count(4), total_action_dqn.count(5), total_action_dqn.count(6)]\n",
        "plot_bar(actions_count_dqn, len(actions_count_dqn), \"ActionsBarDQN\"+str(num_of_evaluation_step))\n",
        "plot_graph(total_reward_dqn, filename=\"RewardGraphDQN\"+str(num_of_evaluation_step))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXextOYje9E"
      },
      "source": [
        "plot_multi_graph(total_reward_dqn, total_reward_a2c, filename=\"RewardGraphCompare\")\n",
        "plot_mult_bar(actions_count_dqn, actions_count_a2c, len(actions_count_dqn), filename=\"ActionBarCompare\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7bjCnnvnRgP"
      },
      "source": [
        "## Video Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "k-G_mwb3o4u-"
      },
      "source": [
        "ac_best_model = ACAgent()\n",
        "ac_filename = \"AC_best_weights\"\n",
        "ac_best_model.load_model(ac_filename)\n",
        "create_video(environment, ac_best_model, \"AC_Best\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aCs0imbKo4u_"
      },
      "source": [
        "dqn_best_agent = DQNAgent(1, True)\n",
        "dqn_filename = \"DQN_best_weights\"\n",
        "dqn_best_agent.load_model(dqn_filename)\n",
        "create_video(environment, dqn_best_agent, \"DQN_Best\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}